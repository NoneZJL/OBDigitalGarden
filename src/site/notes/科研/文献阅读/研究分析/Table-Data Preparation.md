---
{"dg-publish":true,"permalink":"/科研/文献阅读/研究分析/Table-Data Preparation/"}
---

#数据准备 #表格 

>[!abstract] 论文概述 [Lost in the Pipeline: How Well Do Large Language Models Handle Data Preparation?](https://arxiv.org/abs/2511.21708)
>
>数据准备，是测试大语言模型能力的任务中的数据驱动流程中的一个关键但常常需要较多劳动的步骤。
>
>文章探讨大语言模型（包括通用模型和微调模型）能否有效支持用户选择和自动化数据准备任务。使用质量较差的数据集提示这些模型，测量他们执行数据分析和清理等任务的能力。

## Introduction

大多数评估大语言模型应用的文献研究主要关注非结构化数据（文本或图像），而用表格数据测试其应用仍大多未被充分探索。

鉴于数据中心人工智能日益重要，数据质量和数据准备在实现可靠数据分析结果中关键作用被更加强调，需要评估大语言模型在支持和自动化数据分析流程各阶段的潜力。

这类流程中最具挑战性的阶段是数据准备，包括数据分析、转换和清理等任务——这是一个耗时的过程，可能占数据科学家总时间的约 80%。

文章旨在探讨大语言模型在促进表格数据准备方面的有效性：

1. 首先，为了测试大语言模型的能力，文章设计了一个质量模型，结合了传统数据质量指标（如准确性和完备性）与新引入的指标，代表评估大语言模型答案时应该考虑的相关方面。
2. 其次，文章提示大语言模型使用包含先前注入错误信息的数据集解决一组数据准备任务。

文章测试了通用大于语言模型（如 GPT-4、Claude、Gemini、Llama、DeepSeek）和专用表格大型语言模型（如 TableGPT2、TableLLM），以评估它们在自动化以下数据准备任务中的有效性：数据分析、依赖发现、数据整理（如重命名、拆分、合并列）和数据清理。同时还单独分析数据清理子任务，如数据标准化（如组合字段的重组、数据类型检查或用单一拼写替换替代拼写）、数据补补、异常值检测和数据重复处理。每个指标都关联到一份陈述列表（即检查表），手动核对以评估答案质量。

## Workflow

**污染数据集准备**

使用来自 Kaggle 仓库的 100 行样本，包含美国房地产的交易数据。选择该数据集是因其列类型多样（类别、数字和日期）且缺少非结构化内容（如文本）。

从原始数据集中，生成了多个污染版本（注入误差占 10%、30% 和 50%），并使用任务特定的污染函数修改数据：

1. *异常值注入*（用于评估异常值检测）：在数值列中插入异常值来模型异常。
2. *去标准化注入*（用于评估数据标准化）：在特定数据格式列中引入数据格式不一致数据
3. *缺失数据注入*（用于评估数据补缺）：在数值列和类别列中插入缺失值，使用不同的表示方式，如空字符串、nan、-1 或 Unknown。
4. *重复注入*（用于评估数据去重）：通过替换现有行引入完全重复和不完全重复，其中不完全重复包括人工引入的错别字。
5. *结构性问题注入*（用于评估数据整理）：插入非最优结构化数据，包括列拼接、列分裂、列冗余和命名不一致。
6. *依赖注入*（用于评估依赖发现）：修改数据，注入错误的函数依赖，旨在测试大语言模型检测这些依赖的能力。【这部分可以认为是表格中原来的列直接存在一定的依赖关系，比如原表中存在城市和邮编两列，那么这两列就是相互依赖的，同一个城市的邮编肯定是一样的，然后通过手动修改城市或者邮编来破坏这种依赖关系】

为了评估数据分析和清理任务，文章应用了前 4 种污染函数。污染过程从重复注入开始，随后采用基于掩码的方法，选择性地在表格的每个目标位置注入单一类型的误差，避免对同一值应用多个污染函数。

**质量模型定义**

为了评估大语言模型的文本输出，文章设计了一个既包含传统（即准确性和完备性）又包含新方面（如规范性、准备性和具体性）的质量模型，具体的这些度量如下：

- *完备性（COM）*：指大语言模型输出中包含所有解决问题所需的信息片段的程度。
- *准确性（ACC）*：指大语言模型输出中包含的信息与真实数据一致的程度。
- *规范性（PRE）*：指大语言模型输出为用户提供解决问题时清晰且明确路径的程度。
- *准备度（REA）*：指大语言模型输出即可用的程度，意味着无需进一步处理即可解决用户的问题。
- *特异性（SPE）*：指大语言模型输出对表格数据集每一列的具体处理程度，而非整体。

![[attenchments/Pasted image 20251210202050.png#pic_center | 400]]
<center>表1：指标与任务的映射</center>

由于任务的特异性，某些指标被排除在特定任务的评估之外。每个任务考虑的指标见表1。数据分析不需要评估特异性，因为它的任务是总结每个列的特征，本就是在列级别运行的。同时，数据分析是描述性任务，也不需要评估规范性。数据标准化不需要评估特异性，因为它也是在列级别处理的。对于数据去重、整理和依赖发现，也不需要评估特异性，因为它们是对整个表格的操作。

**模型推理**

温度设置为 0，并输入相同的提示词：

```text
	Consider this dataset:\n
	{{csv_text}}\n
	Can you do {{task_name}} on it?
```

其中 {{csv_text}} 被 .csv 格式的文本版本替代，{{task_name}} 被分析中的任务名称替换。

**输出评估**

收集大语言模型的输出后，通过一组检查表进行人工评估。每个指标至少有一个具体的检查清单，
虽然依然可能需要更多版本来适应不同的污染水平。每个检查表都包含一系列事实陈述，用户必须核实给出的结果是否与报告的事实相符。这些陈述的评分为 0 到 1：它们可以是二元的（0 = 假，1 = 真）或连续的（基于具体事实）。

需要注意的是，完备性会影响其他指标的评估。事实上，从不同角度验证事实（也就是使用不同指标）只有在答案中存在该事实时才有意义 —— 也就是完备性得分大于零。因此，大语言模型输出中不存在事实的结果在其他指标的检查列表中被标记为“不可评估”。【也就是如果完备性得分为零，其他指标全部不可评估】。

度量是通过真实事实与可评估事实总数的比值计算的。然而，有些清单会对具体事实更重要的陈述进行加权评估：

- 在准确性检查表中，如数据标准化和修补，这些陈述验证 LLM 提出的解决方案是否有效（即可以应用，但不是最优任务，例如对所有列应用标准修补）或最优（例如对不同列应用高级/临时修补技术）。有效解的事实占总分的 80%，而最优解陈述占剩余 20%。
- 在所有准备度检查表中，语句用于验证 LLM 是否提供“代码”或“可用数据”。“代码”语句占总分的 80%，而“现成数据”语句占剩余 20%。【这里代码占比高是因为具体的任务是让模型提供代码来修复错误的数据】

特异性通过测量 LLM 为特定语句临时提出的解数来评估（例如，“为列 C 提出了一个 NaN 值的推入解”）。总体的特异性得分通过对所有陈述的平均值得到。

**用户研究**

为了验证新引入的质量指标背后的假设，文章进行了一项包含61名国际参与者的用户研究，其中包括41名专家和20名非专家。

该研究包含一份问卷，要求用户在一组大型语言模型的回答中，以喜好量表评估他们对一组陈述的认同或支持程度。

具体来说，针对规范性 ，文章旨在验证用户偏好规范性答案还是非规范性答案。在准备度方面，评估了结构化数值数据、代码和文本建议在支持数据准备方面的有效性是否被适当排序。最后，针对具体性 ，考察了用户是否偏好具体答案而非一般答案，以及他们是否接受回答的粒度变异（即列特定或一般解决方案）。用户研究的主要发现如下：

*指标的重要性*

用户始终将完备性和准确性视为评估回答的关键因素。然而，准备度显示不同专长用户的意见差异：经验丰富的用户优先考虑现成输出，而经验较少的用户则更倾向于获得更多指导来完成任务。 规范性对经验较少的用户更为相关，他们偏好结构化指导，而经验丰富的用户则更倾向于在尝试不同任务时更具灵活性。同样，所有用户都重视特异性 ，但经验丰富的用户对列特定解和泛化解的容忍度更高，而经验较少者则偏好非常具体的解。

*期望的支持水平*：

研究的一个方面关注用户是否更喜欢即用支持，如代码或数值数据，而非文本建议。数值数据成为最受欢迎的格式，带来了最高的满意度。基于代码的支持反响不一，用户对编码的熟悉程度不同，意见不一。最后，基于文本的描述受到一定认可，但普遍被认为不如结构化数据有用。

另一个目标是确定用户是否更倾向于处方支持，即提供单一预定义的任务完成策略，还是多种替代方案，后者需要单独决定具体执行。用户倾向于偏好单一预设策略，但经验也起作用：经验较少的用户寻求结构化指导，而专家则偏好更灵活的选择。

研究还调查了用户是否更喜欢针对具体列的支持，而非更通用的表格式支持。列向解比一般的按表格方法更受青睐。经验更丰富的用户表现出略高的灵活性，但大多数用户更倾向于细粒度、针对特定列的支持。汇总方法的偏好略低，但仍获得了积极反馈。

*模型改进*

用户研究证实，所提质量模型中考虑的所有指标均与用户相关。 完备性 、准确性和特异性无需任何细化。对于规范性 ，虽然根据用户的专业知识提供了不同的视角，文章设想用户会根据自己的经验和需求来解读评分;因此，不改变模型或评分函数。对于准备都 ，文章进行了以下改进：

- 代码建议最初被赋予较高分数，但由于缺乏解释，反馈褒贬不一;因此，评分从 0.8/1 降至 0.5/1。
- 返回清理后的数据集最初评分为 1/1，但用户更倾向于只接收任务相关的元组集合;因此，分数应降至 0.8/1。然而，仅返回部分数据仅适合数据去重，而对于数据补补等任务，这种方法不切实际。该修改仅针对数据去重任务/数据清理子任务实现。

**评估结果**

![[attenchments/Pasted image 20251210205704.png#pic_center]]

*数据分析任务*

这个任务中，GLLM 模型进行表格列的分析操作。总体而言，它们获得了中等完备性分数，意味着预期的分析任务中有一半被报告在答案中。然而由于模型在计数任务上的局限性，准确性评估的结果较差。除 Llama 和 DeepSeek 外，所有 GLLM 模型均无法提供现成的数据信息。

TableLLM 在 TLLM 中表现最佳。然而，在检查其输出后，发现它仅建议使用 pandas 库中的（已弃用的）数据处理工具，

*数据清理任务*

数据清理复杂且包含多个子任务，而 GLLM 只执行其中极其有限的子部分。在文章考虑的子任务中，它们主要执行数据标准化，较少进行数据修补。Gemini 和 DeepSeek 是唯一同时尝试异常值检测且未完全失败的型号。数据去重任务总是被跳过。考虑到数据清理，文章通常观察到随着数据污染程度的增加，LLM 性能会下降。

TLLM 对数据标准化和数据补值的支持有限，对另外两个子任务则完全不支持。注意到 TableLLM 经常完全失败，回答中没有包含任何相关内容。TableGPT2 的回答非常不完整，但至少它们更接近 GLLMS 的回答。

*异常值检测任务*

GLLMS 能够执行异常值检测任务，尽管随着数据集的脏度（即注入异常值数量）增加，其性能会显著下降。Claude 和 DeepSeek 是最有效的 GLLM，始终提供准确、规范且即用的效果。

TLLM 的完备性显著较低，因为它们最多关注五列中的两列（含离群值）。虽然 TableLLM 的整体性能尚可接受，但除了完备性外，TableGPT2 随着异常值数量增加表现下降：它检测到一半甚至一个都检测不到。

*数据标准化任务*

所有大型语言模型在此任务中都表现出稳定的优秀准确性 ，而完备性略低。GPT 通常执行数据规范化（例如缩放）而非数据标准化，导致完备性较低。

TLLM 显示出较差的数据标准化能力：TableGPT2 将数据标准化与数据归一化混淆，而 TableLLM 仅正确解决了注入中的少数标准化问题。

*数据补缺任务*

GLLMS 在数据补充方面展现了显著的能力。即使数据集中脏污度不同（即缺失值数量不同），它们的完备性和准确性评分也相似。虽然所有 GLLM 在其标准格式（如空字符串）中都能有效检测缺失值，但在更复杂的格式（如 -1 或特殊字符如 --）时则难以检测。

TLLMs 对数据补值的支持不足，导致完备性和准确性均为中等水平。此外，TLLM 仅检测简单的缺失值（例如空字符串）。

*数据去重任务*

GLLMS 在检测精确重复方面表现出可靠且最优的性能。然而，当处理非完全相同的重复时，它们的性能会下降，而这些重复几乎不会被检测到。除 DeepSeek 外，规范性表现优异，准备度几乎总是保持高分，意味着他们提供处方性答案和即用数据。

TLLM 从不考虑非完全相同的重复。虽然除完备性和准备度外，所有指标都非常优秀，但无法识别非完全相同的复制品凸显了大型语言模型在支持这一任务上的无用性。

*数据整理任务*

GLLM 有效支持数据整理，建议拆分和合并列，有时还建议删除不必要的列。虽然大多数 GLLMS 正确执行合并和拆分作，但它们的建议并不总是最优的。

TLLMs 在处理数据整理任务方面能力有限。虽然 TableGPT2 主要建议与数据清理相关的任务，但 TableLLM 通过解决污染函数引入的四个结构性问题中的至少一个（即列连接、列拆分、冗余列加法以及代理信息的命名不一致）表现略优。

*依赖发现任务*

GLLM 和 TLLM 都无法完成这项任务，有时甚至明确表示无法完成。

